{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c744c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Preparing tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:00<00:00, 339.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- Configuration ---\n",
    "N_STEPS = 15\n",
    "VOCAB_SIZE = 5000\n",
    "N_TRIALS = 5\n",
    "N_SPLITS = 3\n",
    "EPOCHS_PER_FOLD = 30\n",
    "\n",
    "# === 1. Load and Preprocess Data ===\n",
    "print(\"Loading and preprocessing data...\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/cleaned/LSAPP_Processed.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess timestamps\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['hour_of_day'] = df['timestamp'].dt.hour\n",
    "df['weekday'] = df['timestamp'].dt.weekday\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
    "\n",
    "# Fill missing numerical values\n",
    "if 'time_since_last_app' not in df.columns:\n",
    "    df['time_since_last_app'] = 0\n",
    "if 'is_weekend' not in df.columns:\n",
    "    df['is_weekend'] = df['weekday'] >= 5\n",
    "\n",
    "df = df[['user_id', 'app_name', 'timestamp', 'hour_sin', 'hour_cos', 'time_since_last_app', 'is_weekend', 'weekday']]\n",
    "\n",
    "# Filter out rare apps\n",
    "app_counts = df['app_name'].value_counts()\n",
    "common_apps = app_counts[app_counts >= 40].index\n",
    "df_filtered = df[df['app_name'].isin(common_apps)].copy()\n",
    "\n",
    "# === 2. Tokenizer Setup ===\n",
    "print(\"Preparing tokenizer...\")\n",
    "all_app_sequences = df_filtered.groupby('user_id')['app_name'].apply(lambda x: ' '.join(x)).tolist()\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_app_sequences)\n",
    "word_index = tokenizer.word_index\n",
    "oov_index = word_index.get(tokenizer.oov_token)\n",
    "vocab_size = min(len(word_index) + 1, VOCAB_SIZE)\n",
    "\n",
    "# === 3. Create Sequences with Additional Features ===\n",
    "def create_sequences(data, tokenizer, n_steps, oov_index):\n",
    "    sequences, targets = [], []\n",
    "    extra_features = []\n",
    "    data = data.sort_values(['user_id', 'timestamp'])\n",
    "    for user_id, user_data in tqdm(data.groupby('user_id')):\n",
    "        apps = user_data['app_name'].tolist()\n",
    "        app_tokens = tokenizer.texts_to_sequences([apps])[0]\n",
    "        hour_sin = user_data['hour_sin'].values\n",
    "        hour_cos = user_data['hour_cos'].values\n",
    "        time_since = user_data['time_since_last_app'].fillna(0).values\n",
    "        weekend = user_data['is_weekend'].astype(int).values\n",
    "        weekday = user_data['weekday'].values\n",
    "\n",
    "        for i in range(len(app_tokens) - n_steps):\n",
    "            input_seq = app_tokens[i:i+n_steps]\n",
    "            target = app_tokens[i+n_steps]\n",
    "            if target == oov_index:\n",
    "                continue\n",
    "            sequences.append(input_seq)\n",
    "            targets.append(target)\n",
    "            features = [\n",
    "                hour_sin[i+n_steps],\n",
    "                hour_cos[i+n_steps],\n",
    "                time_since[i+n_steps],\n",
    "                weekend[i+n_steps],\n",
    "                weekday[i+n_steps]\n",
    "            ]\n",
    "            extra_features.append(features)\n",
    "    return np.array(sequences), np.array(extra_features), np.array(targets)\n",
    "\n",
    "X_seq, X_extra, y_raw = create_sequences(df_filtered, tokenizer, N_STEPS, oov_index)\n",
    "X_seq_padded = pad_sequences(X_seq, maxlen=N_STEPS, padding='pre', truncating='pre')\n",
    "y = to_categorical(y_raw, num_classes=vocab_size)\n",
    "X_train_seq, X_test_seq, X_train_extra, X_test_extra, y_train, y_test = train_test_split(\n",
    "    X_seq_padded, X_extra, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "y = to_categorical(y_raw, num_classes=vocab_size)\n",
    "\n",
    "# Split the data\n",
    "X_train_seq, X_test_seq, X_train_extra, X_test_extra, y_train, y_test = train_test_split(\n",
    "    X_seq_padded, X_extra, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save train set\n",
    "np.savez_compressed(\"../data/processed/train_data_2.npz\",\n",
    "                    X_seq=X_train_seq,\n",
    "                    X_extra=X_train_extra,\n",
    "                    y=y_train)\n",
    "\n",
    "# Save test set\n",
    "np.savez_compressed(\"../data/processed/test_data_2.npz\",\n",
    "                    X_seq=X_test_seq,\n",
    "                    X_extra=X_test_extra,\n",
    "                    y=y_test)\n",
    "\n",
    "# Save tokenizer separately if needed\n",
    "import pickle\n",
    "with open(\"../data/processed/tokenizer_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Also save vocab info\n",
    "np.savez_compressed(\"../data/processed/meta_2.npz\",\n",
    "                    vocab_size=np.array([vocab_size]),\n",
    "                    oov_index=np.array([oov_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c14055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyperparameters: {'embedding_dim': 64, 'lstm_units': 128, 'dense_units': 256, 'dropout_rate': 0.36059802347094455, 'learning_rate': 0.00046817343838393284}\n",
      "Training model with fixed hyperparameters...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Load training data\n",
    "train = np.load(\"../data/processed/train_data_2.npz\")\n",
    "X_train_seq = train['X_seq']\n",
    "X_train_extra = train['X_extra']\n",
    "y_train = train['y']\n",
    "\n",
    "# Load test data\n",
    "test = np.load(\"../data/processed/test_data_2.npz\")\n",
    "X_test_seq = test['X_seq']\n",
    "X_test_extra = test['X_extra']\n",
    "y_test = test['y']\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"../data/processed/tokenizer_2.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Load metadata\n",
    "meta = np.load(\"../data/processed/meta_2.npz\")\n",
    "vocab_size = int(meta['vocab_size'][0])\n",
    "oov_index = int(meta['oov_index'][0])\n",
    "\n",
    "# === Load hyperparameters from JSON ===\n",
    "with open(\"Hyperparameters.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Using hyperparameters:\", best_params)\n",
    "\n",
    "# === Fixed model creation function using loaded hyperparameters ===\n",
    "def create_model_with_params():\n",
    "    embedding_dim = best_params['embedding_dim']\n",
    "    lstm_units = best_params['lstm_units']\n",
    "    dense_units = best_params['dense_units']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    # Sequence input\n",
    "    seq_input = Input(shape=(N_STEPS,), name='sequence_input')\n",
    "    x_seq = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(seq_input)\n",
    "    x_seq = LSTM(lstm_units, dropout=dropout_rate)(x_seq)\n",
    "\n",
    "    # Extra numeric input\n",
    "    extra_input = Input(shape=(5,), name='extra_features')\n",
    "\n",
    "    # Combine\n",
    "    x = Concatenate()([x_seq, extra_input])\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[seq_input, extra_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', TopKCategoricalAccuracy(k=3), TopKCategoricalAccuracy(k=5)])\n",
    "    return model\n",
    "\n",
    "# === Direct model training (skip Optuna) ===\n",
    "print(\"Training model with fixed hyperparameters...\")\n",
    "model = create_model_with_params()\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_seq, X_train_extra], y_train,\n",
    "          validation_data=([X_test_seq, X_test_extra], y_test),\n",
    "          epochs=100,\n",
    "          batch_size=512, # <--- This is the cause of all your problems\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "          verbose=1)\n",
    "\n",
    "# === Evaluation ===\n",
    "eval_results = model.evaluate([X_test_seq, X_test_extra], y_test)\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "for name, val in zip(model.metrics_names, eval_results):\n",
    "    print(f\"{name}: {val:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict([X_test_seq, X_test_extra])\n",
    "y_pred_proba[:, oov_index] = 0  # Mask <OOV>\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "target_names = [reverse_word_index.get(i, '<UNK>') for i in sorted(set(np.concatenate([y_pred, y_true])))]\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm[:15, :15], annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (Top 15 Classes)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"App Predictor Model.keras\")\n",
    "print(\"Model saved as 'App Predictor Model.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
